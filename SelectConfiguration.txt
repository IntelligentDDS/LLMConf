vLLM is an open-source high-speed inference framework for large language models. The following three sections form a knowledge base closely related to vLLM configuration parameters, sourced from the official vLLM documentation, the vLLM open-source code and the vLLM project's Issues.

<vLLM Engine Arguments>
Below, you can find an explanation of every engine argument for vLLM:
usage: vllm serve [-h] [--model MODEL] [--tokenizer TOKENIZER]
                  [--skip-tokenizer-init] [--revision REVISION]
                  [--code-revision CODE_REVISION]
                  [--tokenizer-revision TOKENIZER_REVISION]
                  [--tokenizer-mode {auto,slow,mistral}] [--trust-remote-code]
                  [--download-dir DOWNLOAD_DIR]
                  [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral}]
                  [--config-format {auto,hf,mistral}]
                  [--dtype {auto,half,float16,bfloat16,float,float32}]
                  [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]
                  [--quantization-param-path QUANTIZATION_PARAM_PATH]
                  [--max-model-len MAX_MODEL_LEN]
                  [--guided-decoding-backend {outlines,lm-format-enforcer}]
                  [--distributed-executor-backend {ray,mp}] [--worker-use-ray]
                  [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                  [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                  [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]
                  [--ray-workers-use-nsight] [--block-size {8,16,32}]
                  [--enable-prefix-caching] [--disable-sliding-window]
                  [--use-v2-block-manager]
                  [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS] [--seed SEED]
                  [--swap-space SWAP_SPACE] [--cpu-offload-gb CPU_OFFLOAD_GB]
                  [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
                  [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]
                  [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
                  [--max-num-seqs MAX_NUM_SEQS] [--max-logprobs MAX_LOGPROBS]
                  [--disable-log-stats]
                  [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}]
                  [--rope-scaling ROPE_SCALING] [--rope-theta ROPE_THETA]
                  [--enforce-eager]
                  [--max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE]
                  [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]
                  [--disable-custom-all-reduce]
                  [--tokenizer-pool-size TOKENIZER_POOL_SIZE]
                  [--tokenizer-pool-type TOKENIZER_POOL_TYPE]
                  [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]
                  [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT] [--enable-lora]
                  [--max-loras MAX_LORAS] [--max-lora-rank MAX_LORA_RANK]
                  [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]
                  [--lora-dtype {auto,float16,bfloat16,float32}]
                  [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]
                  [--max-cpu-loras MAX_CPU_LORAS] [--fully-sharded-loras]
                  [--enable-prompt-adapter]
                  [--max-prompt-adapters MAX_PROMPT_ADAPTERS]
                  [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]
                  [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu}]
                  [--num-scheduler-steps NUM_SCHEDULER_STEPS]
                  [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]
                  [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]
                  [--speculative-model SPECULATIVE_MODEL]
                  [--speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}]
                  [--num-speculative-tokens NUM_SPECULATIVE_TOKENS]
                  [--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]
                  [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]
                  [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]
                  [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]
                  [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]
                  [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]
                  [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]
                  [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]
                  [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]]
                  [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                  [--ignore-patterns IGNORE_PATTERNS]
                  [--preemption-mode PREEMPTION_MODE]
                  [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]
                  [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]
                  [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]
                  [--collect-detailed-traces COLLECT_DETAILED_TRACES]
                  [--disable-async-output-proc]
                  [--override-neuron-config OVERRIDE_NEURON_CONFIG]

The following are the meanings and default values of the parameters:             
[--model MODEL]:Name or path of the huggingface model to use. Default: “facebook/opt-125m”
[--tokenizer TOKENIZER]:Name or path of the huggingface tokenizer to use. If unspecified, model name or path will be used.
[--skip-tokenizer-init]:Skip initialization of tokenizer and detokenizer
[--revision REVISION]:The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.
[--code-revision CODE_REVISION]:The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.
[--tokenizer-revision TOKENIZER_REVISION]:Revision of the huggingface tokenizer to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.
[--tokenizer-mode {auto,slow,mistral}]:Possible choices: auto(“auto” will use the fast tokenizer if available), slow(“slow” will always use the slow tokenizer), mistral(“mistral” will always use the mistral_common tokenizer).Default: “auto”.
[--trust-remote-code]:Trust remote code from huggingface.
[--download-dir DOWNLOAD_DIR]:Directory to download and load the weights, default to the default cache dir of huggingface.
[--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral}]:The format of the model weights to load.Possible choices: auto(“auto” will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.), pt(“pt” will load the weights in the pytorch bin format.), safetensors(“safetensors” will load the weights in the safetensors format.), npcache(“npcache” will load the weights in pytorch format and store a numpy cache to speed up the loading.), dummy(“dummy” will initialize the weights with random values, which is mainly for profiling.), tensorizer(“tensorizer” will load the weights using tensorizer from CoreWeave. See the Tensorize vLLM Model script in the Examples section for more information.), sharded_state, gguf, bitsandbytes(“bitsandbytes” will load the weights using bitsandbytes quantization.), mistral.Default: “auto”.
[--config-format {auto,hf,mistral}]:The format of the model config to load.Possible choices: auto(“auto” will try to load the config in hf format if available else it will try to load in mistral format), hf, mistral.Default: “auto”.
[--dtype {auto,half,float16,bfloat16,float,float32}]:Data type for model weights and activations.Possible choices: auto(“auto” will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.), half(“half” for FP16. Recommended for AWQ quantization.), float16(“float16” is the same as “half”.), bfloat16(“bfloat16” for a balance between precision and range.), float(“float” is shorthand for FP32 precision.), float32(“float32” for FP32 precision.).Default: “auto”.
[--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]:Possible choices: auto, fp8, fp8_e5m2, fp8_e4m3.Data type for kv cache storage. If “auto”, will use model data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. ROCm (AMD GPU) supports fp8 (=fp8_e4m3).Default: “auto”.
[--quantization-param-path QUANTIZATION_PARAM_PATH]:Path to the JSON file containing the KV cache scaling factors. This should generally be supplied, when KV cache dtype is FP8. Otherwise, KV cache scaling factors default to 1.0, which may cause accuracy issues. FP8_E5M2 (without scaling) is only supported on cuda versiongreater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead supported for common inference criteria.
[--max-model-len MAX_MODEL_LEN]:Model context length. If unspecified, will be automatically derived from the model config.
[--guided-decoding-backend {outlines,lm-format-enforcer}]:Possible choices: outlines, lm-format-enforcer.Which engine will be used for guided decoding (JSON schema / regex etc) by default. Currently support https://github.com/dottxt-ai/outlines and https://github.com/noamgat/lm-format-enforcer. Can be overridden per request via guided_decoding_backend parameter.Default: “outlines”.
[--distributed-executor-backend {ray,mp}]: Possible choices: ray, mp.Backend to use for distributed serving. When more than 1 GPU is used, will be automatically set to “ray” if installed or “mp” (multiprocessing) otherwise.
[--worker-use-ray]:Deprecated, use –distributed-executor-backend=ray.
[--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]:Number of pipeline stages.Default: 1.
[--tensor-parallel-size TENSOR_PARALLEL_SIZE]:Number of tensor parallel replicas.Default: 1.
[--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]:Load model sequentially in multiple batches, to avoid RAM OOM when using tensor parallel and large models.
[--ray-workers-use-nsight]:If specified, use nsight to profile Ray workers.
[--block-size {8,16,32}]:Possible choices: 8, 16, 32.Token block size for contiguous chunks of tokens. This is ignored on neuron devices and set to max-model-len.Default: 16.
[--enable-prefix-caching]: Enables automatic prefix caching.
[--disable-sliding-window]:Disables sliding window, capping to sliding window size.
[--use-v2-block-manager]:Use BlockSpaceMangerV2.
[--num-lookahead-slots NUM_LOOKAHEAD_SLOTS]: Experimental scheduling config necessary for speculative decoding. This will be replaced by speculative config in the future; it is present to enable correctness tests until then.Default: 0.
[--seed SEED]:Random seed for operations.Default: 0.
[--swap-space SWAP_SPACE]:CPU swap space size (GiB) per GPU.Default: 4.
[--cpu-offload-gb CPU_OFFLOAD_GB]:The space in GiB to offload to CPU, per GPU. Default is 0, which means no offloading. Intuitively, this argument can be seen as a virtual way to increase the GPU memory size. For example, if you have one 24 GB GPU and set this to 10, virtually you can think of it as a 34 GB GPU. Then you can load a 13B model with BF16 weight,which requires at least 26GB GPU memory. Note that this requires fast CPU-GPU interconnect, as part of the model isloaded from CPU memory to GPU memory on the fly in each model forward pass.Default: 0.
[--gpu-memory-utilization GPU_MEMORY_UTILIZATION]:The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. For example, a value of 0.5 would imply 50% GPU memory utilization. If unspecified, will use the default value of 0.9.Default: 0.9.
[--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]:If specified, ignore GPU profiling result and use this numberof GPU blocks. Used for testing preemption.
[--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]:Maximum number of batched tokens per iteration.
[--max-num-seqs MAX_NUM_SEQS]:Maximum number of sequences per iteration.Default: 256.
[--max-logprobs MAX_LOGPROBS]:Max number of log probs to return logprobs is specified in SamplingParams.Default: 20.
[--disable-log-stats]:Disable logging statistics.
[--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}]:Possible choices: aqlm, awq, deepspeedfp, tpu_int8, fp8, fbgemm_fp8, modelopt, marlin, gguf, gptq_marlin_24, gptq_marlin, awq_marlin, gptq, compressed-tensors, bitsandbytes, qqq, experts_int8, neuron_quant, None.Method used to quantize the weights. If None, we first check the quantization_config attribute in the model config file. If that is None, we assume the model weights are not quantized and use dtype to determine the data type of the weights.
[--rope-scaling ROPE_SCALING]:RoPE scaling configuration in JSON format. For example, {“type”:”dynamic”,”factor”:2.0}.
[--rope-theta ROPE_THETA]:RoPE theta. Use with rope_scaling. In some cases, changing the RoPE theta improves the performance of the scaled model.
[--enforce-eager]:Always use eager-mode PyTorch. If False, will use eager mode and CUDA graph in hybrid for maximal performance and flexibility.
[--max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE]:Maximum context length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode. (DEPRECATED. Use –max-seq-len-to-capture instead).
[--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]:Maximum sequence length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode.Default: 8192.
[--disable-custom-all-reduce]:See ParallelConfig.
[--tokenizer-pool-size TOKENIZER_POOL_SIZE]:Size of tokenizer pool to use for asynchronous tokenization. If 0, will use synchronous tokenization.Default: 0.
[--tokenizer-pool-type TOKENIZER_POOL_TYPE]:Type of tokenizer pool to use for asynchronous tokenization. Ignored if tokenizer_pool_size is 0.Default: “ray”.
[--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]:Extra config for tokenizer pool. This should be a JSON string that will be parsed into a dictionary. Ignored if tokenizer_pool_size is 0.
[--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]:For each multimodal plugin, limit how many input instances to allow for each prompt. Expects a comma-separated list of items, e.g.: image=16,video=2 allows a maximum of 16 images and 2 videos per prompt. Defaults to 1 for each modality.
[--enable-lora]:If True, enable handling of LoRA adapters.
[--max-loras MAX_LORAS]:Max number of LoRAs in a single batch.Default: 1.
[--max-lora-rank MAX_LORA_RANK]:Max LoRA rank.Default: 16.
[--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]:Maximum size of extra vocabulary that can be present in a LoRA adapter (added to the base model vocabulary).Default: 256.
[--lora-dtype {auto,float16,bfloat16,float32}]:Possible choices: auto, float16, bfloat16, float32.Data type for LoRA. If auto, will default to base model dtype.Default: “auto”.
[--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]:Specify multiple scaling factors (which can be different from base model scaling factor - see eg. Long LoRA) to allow for multiple LoRA adapters trained with those scaling factors to be used at the same time. If not specified, only adapters trained with the base model scaling factor are allowed.
[--max-cpu-loras MAX_CPU_LORAS]:Maximum number of LoRAs to store in CPU memory. Must be >= than max_num_seqs. Defaults to max_num_seqs.
[--fully-sharded-loras]:By default, only half of the LoRA computation is sharded with tensor parallelism. Enabling this will use the fully sharded layers. At high sequence length, max rank or tensor parallel size, this is likely faster.
[--enable-prompt-adapter]:If True, enable handling of PromptAdapters.
[--max-prompt-adapters MAX_PROMPT_ADAPTERS]:Max number of PromptAdapters in a batch.Default: 1.
[--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]:Max number of PromptAdapters tokens.Default: 0.
[--device {auto,cuda,neuron,cpu,openvino,tpu,xpu}]:Possible choices: auto, cuda, neuron, cpu, openvino, tpu, xpu.Device type for vLLM execution.Default: “auto”.
[--num-scheduler-steps NUM_SCHEDULER_STEPS]:Maximum number of forward steps per scheduler call.Default: 1.
[--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]:Apply a delay (of delay factor multiplied by previousprompt latency) before scheduling next prompt.Default: 0.0.
[--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]:If set, the prefill requests can be chunked based on the max_num_batched_tokens.
[--speculative-model SPECULATIVE_MODEL]:The name of the draft model to be used in speculative decoding.
[--speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}]:Possible choices: aqlm, awq, deepspeedfp, tpu_int8, fp8, fbgemm_fp8, modelopt, marlin, gguf, gptq_marlin_24, gptq_marlin, awq_marlin, gptq, compressed-tensors, bitsandbytes, qqq, experts_int8, neuron_quant, None.Method used to quantize the weights of speculative model.If None, we first check the quantization_config attribute in the model config file. If that is None, we assume the model weights are not quantized and use dtype to determine the data type of the weights.
[--num-speculative-tokens NUM_SPECULATIVE_TOKENS]:The number of speculative tokens to sample from the draft model in speculative decoding.
[--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]:Number of tensor parallel replicas for the draft model in speculative decoding.
[--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]:The maximum sequence length supported by the draft model. Sequences over this length will skip speculation.
[--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]:Disable speculative decoding for new incoming requests if the number of enqueue requests is larger than this value.
[--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]:Max size of window for ngram prompt lookup in speculative decoding.
[--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]:Min size of window for ngram prompt lookup in speculative decoding.
[--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]:Possible choices: rejection_sampler, typical_acceptance_sampler.Specify the acceptance method to use during draft token verification in speculative decoding. Two types of acceptance routines are supported: 1) RejectionSampler which does not allow changing the acceptance rate of draft tokens, 2) TypicalAcceptanceSampler which is configurable, allowing for a higher acceptance rate at the cost of lower quality, and vice versa.Default: “rejection_sampler”.
[--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]:Set the lower bound threshold for the posterior probability of a token to be accepted. This threshold is used by the TypicalAcceptanceSampler to make sampling decisions during speculative decoding. Defaults to 0.09.
[--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]:A scaling factor for the entropy-based threshold for token acceptance in the TypicalAcceptanceSampler. Typically defaults to sqrt of –typical-acceptance-sampler-posterior-threshold i.e. 0.3.
[--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]]:If set to True, token log probabilities are not returned during speculative decoding. If set to False, log probabilities are returned according to the settings in SamplingParams. If not specified, it defaults to True. Disabling log probabilities during speculative decoding reduces latency by skipping logprob calculation in proposal sampling, target sampling, and after accepted tokens are determined.
[--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]:Extra config for model loader. This will be passed to the model loader corresponding to the chosen load_format. This should be a JSON string that will be parsed into a dictionary.
[--ignore-patterns IGNORE_PATTERNS]:The pattern(s) to ignore when loading the model.Default to ‘original/**/*’ to avoid repeated loading of llama’s checkpoints.Default: [].
[--preemption-mode PREEMPTION_MODE]:If ‘recompute’, the engine performs preemption by recomputing; If ‘swap’, the engine performs preemption by block swapping.
[--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]:The model name(s) used in the API. If multiple names are provided, the server will respond to any of the provided names. The model name in the model field of a response will be the first name in this list. If not specified, the model name will be the same as the –model argument. Noted that this name(s)will also be used in model_name tag content of prometheus metrics, if multiple names provided, metricstag will take the first one.
[--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]:Name or path of the QLoRA adapter.
[--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]:Target URL to which OpenTelemetry traces will be sent.
[--collect-detailed-traces COLLECT_DETAILED_TRACES]:Valid choices are model,worker,all. It makes sense to set this only if –otlp-traces-endpoint is set. If set, it will collect detailed traces for the specified modules. This involves use of possibly costly and or blocking operations and hence might have a performance impact.
[--disable-async-output-proc]:Disable async output processing. This may result in lower performance.
[--override-neuron-config OVERRIDE_NEURON_CONFIG]:override or set neuron device configuration.
</vLLM Engine Arguments>

<vLLM Source Code Config Relations>
Below is a summary of the relationships among configuration parameters derived from vLLM's open-source code:
[1] If max-num-batched-tokens is not explicitly set, the system dynamically calculates it based on max-num-seqs and block-size (default: max-num-seqs * block-size).
[2] block-size must be a factor of max-model-len. If max-model-len % block-size != 0, a ValueError will be raised.
[3] When kv-cache-dtype is set to fp8 or its subtypes (e.g., fp8_e4m3), the quantization-param-path parameter must be provided. Otherwise, a warning will be triggered.
[4] gpu-memory-utilization directly affects the block allocation strategy. If the utilization is too high (close to 1.0), it may trigger an OOM, at which point the system will attempt to use the CPU swap space defined by swap-space.
[5] When LoRA is enabled (enable-lora=True), max-logprobs is forced to 0.
[6] When using speculative decoding (speculative-model), num-speculative-tokens must be greater than 0 and speculative-max-model-len cannot be less than the main model's max-model-len.
[7] When running on non-CUDA devices (such as CPUs and TPUs), some quantization methods (e.g., bitsandbytes, fp8) will be disabled.
[8] If max-model-len is not explicitly set, it will be extracted from the HuggingFace model configuration's max-position-embeddings or model-max-length field.
[9] use-v2-block-manager and enable-prefix-caching cannot be enabled simultaneously. The V2 block manager already has built-in prefix caching logic.
[10] For models with built-in sliding windows (e.g., Mistral), explicitly setting disable-sliding-window=True will override the model's original configuration.
[11] When using guided-decoding-backend=lm-format-enforcer, max-logprobs will be forced to 0 (because the format enforcement engine requires exclusive handling of logprobs).
[12] The product of pipeline-parallel-size and tensor-parallel-size cannot exceed the total number of GPUs.
[13] When using load-format=sharded_state, this value is automatically set to tensor-parallel-size to ensure shards are loaded correctly.
[14] num-lookahead-slots must be less than or equal to block-size. Otherwise, a ValueError is raised.
[15] When both cpu-offload-gb and swap-space are greater than 0, the total CPU memory pool size is swap-space + cpu-offload-gb, which is used for hybrid swapping and offloading.
[16] If speculative-max-model-len is not explicitly set, it defaults to the main model's max-model-len.
[17] When PromptAdapter is enabled, max-prompt-adapter-token must be greater than 0, otherwise a ValueError is raised.
[18] The format parsing for long-lora-scaling-factors: the input must be a comma-separated list of floating-point numbers, which is then parsed and stored as a List[float].
[19] The content of model-loader-extra-config is merged into the model loader's load-config as a dictionary, taking precedence over other parameters.
[20] If ngram-prompt-lookup-min is not explicitly set, the default value is max(1, ngram-prompt-lookup-max // 2).
[21] fully-sharded-loras only takes effect when tensor-parallel-size > 1; otherwise, this parameter is silently ignored.
[22] override-neuron-config only takes effect when device=neuron; setting this parameter on other device types will trigger a warning.
[23] When the number of pending requests exceeds the threshold defined by speculative-disable-by-batch-size, speculative decoding is disabled, but already initiated speculative batches will continue to execute.
[24] At startup, a buffer of size max-prompt-adapter-token * embedding-dim is pre-allocated to avoid dynamic adjustments at runtime.
[25] If max-cpu-loras is not explicitly set, the default value is max(max-num-seqs, 1) to ensure that at least one sequence can be processed.
[26] If speculative-draft-tensor-parallel-size is not specified, it defaults to the main model's tensor-parallel-size.
[27] model-loader-extra-config only allows predefined keys (e.g., "load_weights" and "skip_modules"); any invalid keys will be filtered out.
[28] When num-scheduler-steps is set to a value greater than 1, multiple forward passes per scheduling round are forced to test stability under extreme load.
[29] max-num-seqs < max-num-batched-tokens (enforced in AsyncEngineDeadline scheduling logic).
[30] On Neuron devices, block-size is overridden to equal max-model-len (ignores user input).
[31] If num-gpu-blocks-override is set, GPU profiling is skipped, and gpu-memory-utilization is ignored.
[32] enable-lora must be True for max-loras to take effect.
[33] When kv-cache-dtype is set to FP8 variants, quantization-param-path must be provided.
[34] fp8_e5m2 is only allowed on CUDA ≥11.8 and fp8_e4m3 is enforced on ROCm (AMD).
[35] If unspecified, speculative-draft-tensor-parallel-size inherits from tensor-parallel-size.
[36] max-cpu-loras defaults to max-num-seqs if unset and must be ≥ max-num-seqs.
[37] Prefix caching is disabled if use-v2-block-manager is enabled.
[38] Total parallelism (pipeline-parallel-size * tensor-parallel-size) must ≤ total GPUs.
[39] Both contribute to CPU memory reservations, but cpu-offload-gb prioritizes offloaded weights.
[40] num-speculative-tokens is capped at max-num-batched-tokens // 2 to avoid OOM.
[41] lm-format-enforcer requires max-logprobs ≥ 1 to function.
[42] load-format="tensorizer" is incompatible with quantization methods (raises NotImplementedError).
[43] When sliding window is disabled, context window is hard-capped to the model’s max-position-embeddings.
[44] rope-theta is ignored unless rope-scaling is explicitly configured.
[45] enforce-eager=True disables CUDA graph capture regardless of max-seq-len-to-capture.
[46] tokenizer-pool-type is ignored if tokenizer-pool-size=0 (synchronous mode).
[47] LongLoRA scaling factors override rope-scaling if both are specified.
[48] Speculative decoding is skipped if the batch size exceeds this threshold (avoids latency spikes).
[49] min must be ≤ max and both must be ≥1 (enforced during config validation).
[50] Prefill requests are split into chunks sized to max-num-batched-tokens (reduces memory spikes).
[51] Both main and draft models must use the same device type (e.g., CUDA).
[52] worker-use-ray is deprecated and forces distributed-executor-backend=ray.
[53] preemption-mode="swap" requires contiguous physical GPU memory for block swapping.
[54] The JSON config is passed to the loader specified by load-format (e.g., tensorizer).
[55] Values like image=16 are parsed into a dict {"image": 16} for per-modality limits.
[56] If unset, alpha = sqrt(posterior-threshold) (e.g., 0.3 for threshold 0.09).
[57] max-prompt-adapters is ignored unless enable-prompt-adapter=True.
[58] Draft model’s max-model-len must ≥ main model’s max-model-len (enforced during loading).
[59] typical-acceptance-sampler requires posterior-threshold and posterior-alpha parameters.
</vLLM Source Code Config Relations>

<Related Issues>
Below are the configuration parameter-related issues filtered from all issues in the vLLM project, with each issue providing its number and main content:
[#13387] Enabling Chunk Prefill on the ppc64le (IBM POWER) platform (i.e. when using the --enable-chunked-prefill parameter) fails due to an ipex dependency issue.
[#12260] Removed incorrect comments, fixing the issue where the dependencies for outlines and compressed-tensors were stripped due to the comments. Note that "compressed-tensors" is an optional parameter for --quantization.
[#12258] Reported an issue where, when using deepseek v3, it is not possible to set --tensor-parallel-size=16 and --pipeline-parallel-size=2 on the NVIDIA L20.
[#11745] Reported an issue where setting a large context length (--max-model-len) when using vLLM leads to GPU memory exhaustion.
[#11644] When using the vllm serve command, users specify parameters such as --max-model-len, --quantization (e.g., fp8), --tensor-parallel-size, and --trust-remote-code, but a ModuleNotFoundError is triggered at runtime, causing the model to fail to load.
[#11537] An error occurs when loading models that use FP8/W8A8 quantization methods, with the issue related to the usage of the --quantization parameter.
[#11331] On the POWERPC architecture, when setting --dtype to "auto", the automatic selection fails because FP16 is not supported; this issue is resolved by switching to BF16.
[#11128] An error occurs when deploying the MiniCPM-V_2_6_awq_int4 model, indicating that the weight partition sizes do not meet the requirements. It is recommended to reduce --tensor-parallel-size or switch to --quantization gptq.
[#10923] When using the deepseek v2.5 model configured with GPTQ (int4) quantization, an error occurs, related to the gptq option in the --quantization parameter.
[#9579] Due to asynchronous output processing being enabled by default (disable_async_output_proc=False), long context tests fail; this issue was temporarily resolved by adjusting that parameter.
[#9578] When launching the internvl2-8b model service, using the parameters --dtype auto, --gpu-memory-utilization, and --trust-remote-code causes a CUDA out-of-memory error.
[#9163] When loading the Llama-3.2-11B-Vision-Instruct model, specifying --dtype=bfloat16 leads to excessive memory usage, eventually resulting in an OOM error.
[#8956] Made the scheduling strategy configurable via EngineArgs, allowing relevant scheduling parameters to be set directly when creating the engine.
[#8416] When using the --num-speculative-tokens parameter, vLLM throws an error "Worker.init() got an unexpected keyword argument 'num_speculative_tokens'". Removing this parameter then results in a message that the parameter is required, causing the draft model to fail to load.
[#8313] In offline mode, using guided decoding (such as the outlines backend) for generation significantly reduces speed (10 to 20 times slower than unguided generation), reflecting a performance issue with the relevant guided decoding configuration parameters.
[#8104] Under high request loads, using configuration parameters such as --max_num_batched_tokens, --max_num_seqs, and --rope-scaling results in extremely low throughput and high latency for the vLLM service, indicating a performance degradation issue under these settings.
[#7587] In vLLM 0.5.0, when running inference with prefix caching enabled (enable_prefix_caching=True), an error occurs after processing five consecutive batches of 5000 items, causing the Qwen2 0.5B instruct model to report an error.
[#7586] Proposed adding a benchmark script to measure speculative decoding metrics (such as scoring, verification, proposal time, and acceptance rate), involving configuration parameters related to speculative decoding (e.g., speculative-model, num-speculative-tokens, etc.).
[#7479] In an H100 environment, tests for the aqlm mode fail, resulting in some incorrect generations; the issue is related to the aqlm option in the quantization parameter.
[#7378] When using the facebook/opt-125m model, performing prefill/prefix operations with FP8 enabled (involving the kv-cache-dtype configuration) triggers an illegal memory access error in the Triton kernel.
[#7375] When performing inference with the Mixtral-8x7B-Instruct-v0.1-AWQ model, calling llm.generate returns empty text.
[#7277] Reused the compressed-tensors code to support quantization configuration, adding a dependency on quantization-related parameters (such as the compressed-tensors option in --quantization).
[#6868] Explored the impact of block size on performance; experimental results did not verify the hypothesis that a smaller block size reduces memory fragmentation but increases management overhead.
[#6767] Reported a potential data race issue when running the Llama 405b fp8 model, affecting the stability of the fp8 quantization configuration.
[#6665] Fixed an issue in the FBGEMM fp8 quantization process where modules_to_not_convert being null caused a check to fail.
[#6561] Reported an issue where using configuration parameters such as --max-model-len, --gpu-memory-utilization, --enforce-eager, --seed, and --served-model-name fails to fully utilize the available GPU memory.
[#7175] Extended the block-size parameter to allow it to be set to the sequence length to meet the requirements of neuron devices.
[#6455] Optimized memory usage for pipeline parallel models by creating components based on rank, reducing unnecessary component instances using the --pipeline-parallel-size parameter.
[#6248] Proposed setting --tensor-parallel-size to -1 to automatically use all available CUDA devices, thereby simplifying multi-GPU deployment configuration.
[#6246] Discussed the issue that, under the FP8 data type, attention layer computations need to first convert FP8 to FP16/BF16, and inquired whether there are plans to directly support FP8 attention calculations (related to the fp8 option in the --quantization parameter).
[#6142] Addressed an error in the deepseek-v2 model when using awq quantization, involving the support for setting the --quantization parameter to awq.
[#6038] Pointed out that speculative decoding fails to adhere to the --seed parameter set in each request, resulting in non-reproducible results.
[#5513] Reported an FP8 weight loading error when loading the gpt_bigcode model, involving an issue with the fp8 option in the --quantization parameter.
[#4570] Proposed improvements to address the insufficient robustness of the FP8 static scaling implementation, in order to enhance the stability of FP8 computations.
[#4346] Discussed whether, in Llama2 models, --max_model_len can be set to a value exceeding the --max_position_embeddings specified in the model configuration, and mentioned the possibility of automatic rope scaling.
[#4344] Reported that when loading the mistralai/Mixtral-8x22B-Instruct-v0.1 model, errors occur when using configuration parameters such as --seed, --tensor-parallel-size, --max-num-batched-tokens, and --max-log-len, causing issues in distributed execution.
[#4242] Discussed that when loading multiple models on the same GPU in a cluster environment, using configuration parameters such as --gpu-memory-utilization, --max_model_len, and --enforce-eager can lead to resource allocation errors, thereby affecting model deployment.
[#4139] For the issue of inaccurate prompt logprob calculation when using --enable-chunked-prefill, proposed introducing a new do_sample parameter to correct the logprob computation.
[#4138] Fixed an issue in the LoRA loading check to ensure that the LoRA adapter is loaded correctly, corresponding with parameters such as --enable-lora and --max_loras in the official documentation.
[#3417] Explored whether vLLM supports LoRA inference for wizardcoder and deepseek models, involving the support for LoRA adapter configuration parameters.
[#3416] Reported that when deploying vLLM in Docker, errors occur when using configuration parameters such as --model, --tensor-parallel-size, --dtype, --quantization, --max-model-len, and --enforce-eager, causing the service to fail to start.
[#3316] Tested the throughput of the Qwen1.8B model using merged LoRA weights versus dynamically calling LoRA, with results showing that merging LoRA weights is faster.
[#3315] Implemented LoRA for the Qwen1.5 model, referencing the LoRA implementation in the Llama model.
[#3314] Discussed support for pipeline parallelism, noting that there is currently a preference for tensor parallelism, and welcomed contributions to asynchronous pipeline parallelism.
[#3313] Proposed supporting different sampling parameters for different requests in the LLM API, to allow parallel inference with individually configured sampling parameters.
[#3210] Inquired about the appropriate scenarios for using the npcache weight loading format, noting that it loads more slowly compared to the safetensors format.
[#2994] Proposed the integration of X-LoRA, which requires dual KV caches, and discussed the complexity of implementing this feature in vLLM.
[#2889] Fixed an issue in the LLM entry code regarding LoRA support, ensuring that the LoRA adapter is loaded correctly.
[#2787] Reported that when using squeezellm quantization (in conjunction with --max-model-len, --gpu-memory-utilization, and --quantization squeezellm), GPU memory usage is higher than expected, raising doubts about whether the configuration parameters are effective.
[#2785] Encountered an error when loading the Qwen1.5-7B-Chat model, as the model's maximum sequence length exceeds the KV cache capacity; it is recommended to adjust the --max-model-len or --gpu-memory-utilization parameters.
[#2782] Proposed supporting the setting of logprobs to the size of the model's vocabulary, to address issues arising from differences in vocabulary sizes between models.
[#374] Suggested updating the error message for V100 GPUs, advising users to set --dtype to "half" to avoid the issue.
[#2373] Compared the speed differences between vLLM and trtllm when using awq quantization, questioning the performance of the awq GEMM kernel in vLLM.
[#479] Reported that when using --tensor-parallel-size greater than 1, RayWorker gets stuck in multi-GPU environments, suggesting that this configuration may be problematic in distributed execution.
[#2476] Described an issue where using the Mixtral-8x7B-Instruct-v0.1 model with configuration parameters such as --seed, --tensor-parallel-size, and --max-num-batched-tokens causes generation interruptions or crashes.
[#2374] Suggested providing more user-friendly error messages on V100 GPUs and advising users to set --dtype to half.
[#2479] Reported that when using --tensor-parallel-size greater than 1, RayWorker experiences slowdowns, possibly related to the setting of this parameter.
[#1446] Regarding setting model quantization to int4 via the API, it was noted that vLLM currently does not support 4-bit quantization, involving the --quantization parameter.
[#1445] Failed to load the Qwen-14B-Chat tokenizer; the error message advises setting the --trust-remote-code parameter to load a custom tokenizer.
[#2164] Fixed an issue with the RoPE kernel on long sequences, involving the --rope-scaling and --rope-theta parameters.
[#2065] Added a Neuron build option to allow building vLLM with the Neuron toolchain, related to parameters such as --device.
[#2064] Reported that Mistral/Mixtral models output anomalous results when generating more than 4096 tokens, suggesting that the sliding window configuration (related to the --disable-sliding-window parameter) may need adjustment.
[#2165] Regarding CUDA errors when using GPTQ quantization on V100, it is suggested that adjustments to quantization configuration parameters (such as --quantization) or rebuilding the image might be necessary.
[#1759] Reported an error during single GPU inference due to tensor model parallel group initialization issues, indicating a compatibility problem with the configuration parameters in a single GPU environment.
[#1449] Explored whether increasing --tensor-parallel-size or adjusting the --gpu-memory-utilization and --max-model-len parameters could distribute the memory burden when a single GPU is insufficient.
[#1448] Discussed an issue where, when using the llama2‑70B model, setting --tensor-parallel-size to 8 causes the probability tensors during generation to take on illegal values (such as inf or nan), whereas setting it to 4 works normally; the cause was questioned.
[#609] Confirmed that vLLM supports tensor parallel deployment across multiple single-GPU machines, and recommended referring to the official documentation for distributed deployment configuration.
[#7896] When deploying the Meta-Llama-3.1-405B-Instruct model in a Kubernetes environment, multiple configuration parameters were used in the startup command (such as --model, --tensor-parallel-size, --pipeline_parallel_size, --max-logprobs, --max-num-batched-tokens, --enable-chunked-prefill, --kv-cache-dtype, --disable-log-stats, --gpu-memory-utilization, --device, and --quantization), and one replica repeatedly restarted due to NCCL errors.
[#7793] When running the Mistral-Nemo-Instruct-2407 model, using FP8 quantization led to an out-of-memory issue. The startup command included configuration parameters (such as --model, --max-model-len, --gpu-memory-utilization, --quantization, --enable-prefix-caching, and --enforce-eager), which resulted in a regression error during the FP8 Marlin quantization process.
</Related Issues>

Please identify the parameters from the vLLM Engine Arguments that have the greatest impact on the performance of LLM based on the information above, and provide reasons. Please generate at least ten independent chains of reasoning that lead to your answer, and then provide a final consolidated answer based on the consensus of these reasoning paths.
