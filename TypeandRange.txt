Below is the model configuration information for llama3 based on vllm deployment:

<config>
{
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}
</config>

<configuration>
{"framework":"Pytorch","task":"text-generation"}
</configuration>

<generation_config>
{
  "bos_token_id": 128000,
  "eos_token_id": [128001, 128009],
  "do_sample": true,
  "temperature": 0.6,
  "max_length": 4096,
  "top_p": 0.9,
  "transformers_version": "4.40.0.dev0"
}
</generation_config>

<experimental gpu configuration>
{
    NVIDIA-SMI 550.54.15
    Driver Version: 550.54.15
    CUDA Version: 12.4
    GPU  Name:NVIDIA A40
    Memory-Usage:46068MiB
}
</experimental gpu configuration>

<meaning of max-num-batched-tokens>
[--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]: Maximum number of batched tokens per iteration.
</meaning of max-num-batched-tokens>

<vLLM Source Code Config Relations>
[1] If max-num-batched-tokens is not explicitly set, the system dynamically calculates it based on max-num-seqs and block-size (default: max-num-seqs * block-size).
[2] max-num-seqs < max-num-batched-tokens (enforced in AsyncEngineDeadline scheduling logic).
[3] num-speculative-tokens is capped at max-num-batched-tokens // 2 to avoid OOM.
[4] Prefill requests are split into chunks sized to max-num-batched-tokens (reduces memory spikes).
</vLLM Source Code Config Relations>

<Related Issues>
[#4344] Reported that when loading the mistralai/Mixtral-8x22B-Instruct-v0.1 model, errors occur when using configuration parameters such as --seed, --tensor-parallel-size, --max-num-batched-tokens, and --max-log-len, causing issues in distributed execution.
[#2476] Described an issue where using the Mixtral-8x7B-Instruct-v0.1 model with configuration parameters such as --seed, --tensor-parallel-size, and --max-num-batched-tokens causes generation interruptions or crashes.
[#7896] When deploying the Meta-Llama-3.1-405B-Instruct model in a Kubernetes environment, multiple configuration parameters were used in the startup command (such as --model, --tensor-parallel-size, --pipeline_parallel_size, --max-logprobs, --max-num-batched-tokens, --enable-chunked-prefill, --kv-cache-dtype, --disable-log-stats, --gpu-memory-utilization, --device, and --quantization), and one replica repeatedly restarted due to NCCL errors.
</Related Issues>

Please provide a more precise range for max-num-batched-tokens based on the above. Please generate at least ten independent chains of reasoning that lead to your answer, and then provide a final consolidated answer based on the consensus of these reasoning paths.
